{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# 魔术指令，自动加载模块\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/home/beihang/xihu/HZTourism/FlowPred-dev\")\n",
    "import sys\n",
    "sys.path.append(\"/home/beihang/xihu/HZTourism/FlowPred-dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置项\n",
    "exper_name = \"mse_loss\"\n",
    "exper_dir = f\"exper/{exper_name}\"\n",
    "exper_data_dir = f\"exper_data/{exper_name}\"\n",
    "os.makedirs(exper_data_dir, exist_ok=True)\n",
    "\n",
    "spot_id = 14207\n",
    "his_hour = 24\n",
    "pred_hour = 6\n",
    "\n",
    "raw_dir = f\"{exper_data_dir}/raw/{spot_id}\"\n",
    "proc_dir = f\"{exper_data_dir}/proc/{spot_id}\"\n",
    "train_dir = f\"{exper_data_dir}/train/{spot_id}\"\n",
    "test_dir = f\"{exper_data_dir}/test/{spot_id}\"\n",
    "res_dir = f\"{exper_data_dir}/res/{spot_id}\"\n",
    "\n",
    "\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(proc_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)   \n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(res_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用元组作为键的配置映射\n",
    "spot_len_config_map = {\n",
    "    # 10分钟频率景点\n",
    "    (14210, 14211, 14212, 14213): {\"freq\": \"10min\", \"his_len\": 144, \"pred_len\": 36},\n",
    "    \n",
    "    # 30秒频率景点\n",
    "    (14207, 14209): {\"freq\": \"30sec\", \"his_len\": 2880, \"pred_len\": 720},\n",
    "    \n",
    "    # 1分钟频率景点\n",
    "    (14208,): {\"freq\": \"1min\", \"his_len\": 1440, \"pred_len\": 360},\n",
    "}\n",
    "\n",
    "# 默认配置\n",
    "default_config = {\"freq\": \"5min\", \"his_len\": 288, \"pred_len\": 72}\n",
    "\n",
    "def get_spot_len_config(spot_id):\n",
    "    for spot_tuple, config in spot_len_config_map.items():\n",
    "        if spot_id in spot_tuple:\n",
    "            return config\n",
    "    return default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_len_config = get_spot_len_config(spot_id)\n",
    "his_len = spot_len_config[\"his_len\"]\n",
    "pred_len = spot_len_config[\"pred_len\"]\n",
    "freq = spot_len_config[\"freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据库连接成功！\n",
      "正在从表 'dahua_flow' 中查询数据，时间范围: 2024-07-20 00:00:00 至 2025-07-20 23:59:59...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出表头: ['spot_id', 'kpi_time', 'kpi_value']\n",
      "CSV 文件表头已写入。\n",
      "成功！筛选后的数据已导出到文件 'exper_data/mse_loss/raw/14207/14207_2024-07-20_2025-07-20.csv'，共 1290115 条记录。\n",
      "数据库连接已关闭。\n"
     ]
    }
   ],
   "source": [
    "# 根据景点加载训练数据\n",
    "from src.utils.utils_data import save_csv_from_db\n",
    "\n",
    "s_time = \"2024-07-20\"\n",
    "e_time = \"2025-07-20\"\n",
    "if e_time is None:\n",
    "    file_base_name = f\"{spot_id}_{s_time}\"\n",
    "    train_raw_data_file=f\"{raw_dir}/{file_base_name}.csv\"\n",
    "else:\n",
    "    file_base_name = f\"{spot_id}_{s_time}_{e_time}\"\n",
    "    train_raw_data_file=f\"{raw_dir}/{file_base_name}.csv\"\n",
    "save_csv_from_db(\n",
    "    spot_id=spot_id,\n",
    "    s_time=f\"{s_time} 00:00:00\",\n",
    "    e_time= f\"{e_time} 23:59:59\",\n",
    "    output_csv_file=train_raw_data_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始增强版Koopman预处理 - 景点14207\n",
      "原始数据形状: (1032082, 3)\n",
      "原始问题检查:\n",
      "  变异系数: 1.052\n",
      "  零值数量: 6212\n",
      "  数据范围: [0.0, 719.0]\n",
      "开始逐步处理:\n",
      "  步骤1: 清理NaN和Inf...\n",
      "  步骤2: 激进连续零值处理...\n",
      "    处理了 32 个长连续零值段\n",
      "  步骤3: 强力降低变异性...\n",
      "    当前变异系数: 1.047\n",
      "    应用对数变换进一步降低变异性...\n",
      "    强力平滑后变异系数: 0.195\n",
      "  步骤4: 增强异常值处理...\n",
      "  步骤5: 强化数值稳定性...\n",
      "\n",
      "=== 强化处理结果对比 ===\n",
      "变异系数: 1.052 -> 0.195 (目标: <0.6)\n",
      "零值数量: 6212 -> 0\n",
      "最大连续零值: 120 -> 0\n",
      "数据范围: [0.0, 719.0] -> [44.8, 135.6]\n",
      "动态范围比: 7190.0 -> 3.0\n",
      "✅ 强化处理完全成功！应该能彻底解决NaN问题\n",
      "✅ 强化版Koopman预处理完成\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 数据预处理\n",
    "df = pd.read_csv(train_raw_data_file)\n",
    "# 按kpi_time列转换为datetime格式\n",
    "df['kpi_time'] = pd.to_datetime(df['kpi_time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "# 按kpi_time去重\n",
    "df = df.drop_duplicates(subset=['kpi_time'])\n",
    "# 按kpi_time排序\n",
    "df = df.sort_values(by='kpi_time')\n",
    "# 数据预处理，分景点\n",
    "if spot_id in [14210,14211,14212,14213]:\n",
    "    pass\n",
    "elif spot_id in [14207,14209]:\n",
    "    from src.utils.utils_data import fill_missing_value_singlespot_30s,preprocess_for_koopman_30s_enhanced\n",
    "    df_proc = fill_missing_value_singlespot_30s(spot_id, df)\n",
    "    # 添加Koopman专用预处理\n",
    "    df_proc = preprocess_for_koopman_30s_enhanced(df_proc, spot_id)\n",
    "elif spot_id in [14208]:\n",
    "    pass\n",
    "else:\n",
    "    from src.utils.utils_data import fill_missing_value_singlespot_day\n",
    "    df_proc = fill_missing_value_singlespot_day(df, freq=freq)\n",
    "\n",
    "df_proc.to_csv(f\"{proc_dir}/{file_base_name}_proc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting group annotation...\n",
      "Getting continuous groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building continuous groups: 100%|██████████| 1032082/1032082 [00:57<00:00, 17892.95row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 481 continuous groups\n",
      "Processing groups (his_len=2880, pred_len=720)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing continuous groups: 100%|██████████| 481/481 [11:10<00:00,  1.39s/group] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 470 holiday batches and 937 workday batches\n",
      "Final merging and deduplicating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging batches: 100%|██████████| 94/94 [00:14<00:00,  6.48it/s]\n",
      "Merging batches: 100%|██████████| 19/19 [00:02<00:00,  6.42it/s]\n",
      "Merging batches: 100%|██████████| 4/4 [00:00<00:00,  6.13it/s]\n",
      "Merging batches: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\n",
      "Merging batches: 100%|██████████| 188/188 [00:28<00:00,  6.51it/s]\n",
      "Merging batches: 100%|██████████| 38/38 [00:05<00:00,  6.35it/s]\n",
      "Merging batches: 100%|██████████| 8/8 [00:01<00:00,  6.08it/s]\n",
      "Merging batches: 100%|██████████| 2/2 [00:00<00:00,  5.34it/s]\n",
      "Merging batches: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final holiday data: 443020 rows\n",
      "Final workday data: 744790 rows\n",
      "Getting final continuous groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building continuous groups: 100%|██████████| 443020/443020 [00:25<00:00, 17513.59row/s]\n",
      "Building continuous groups: 100%|██████████| 744790/744790 [00:41<00:00, 17799.60row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final holiday groups: 57\n",
      "Final workday groups: 76\n",
      "Saving 57 groups to mode_0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving mode_0 files: 100%|██████████| 57/57 [00:01<00:00, 40.84file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode_0 data saved successfully!\n",
      "Saving 76 groups to mode_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving mode_1 files: 100%|██████████| 76/76 [00:02<00:00, 32.68file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode_1 data saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据处理为模型输入形式\n",
    "\n",
    "from src.pattern.pattern_train import get_group_annotation, save_mode_data\n",
    "df_proc = pd.read_csv(f\"{proc_dir}/{file_base_name}_proc.csv\")\n",
    "save_base_dir = train_dir\n",
    "os.makedirs(save_base_dir, exist_ok=True)\n",
    "groups_mode_0, groups_mode_1 = get_group_annotation(his_len=his_len,pred_len=pred_len, df=df_proc, time_interval=freq)\n",
    "save_mode_data(\n",
    "    groups_mode=groups_mode_0,\n",
    "    mode=0,\n",
    "    data_basepath=save_base_dir,\n",
    ")\n",
    "save_mode_data(\n",
    "    groups_mode=groups_mode_1,\n",
    "    mode=1,\n",
    "    data_basepath=save_base_dir,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
