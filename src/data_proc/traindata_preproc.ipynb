{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 魔术指令，自动加载模块\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/home/beihang/xihu/HZTourism/FlowPred-dev\")\n",
    "import sys\n",
    "sys.path.append(\"/home/beihang/xihu/HZTourism/FlowPred-dev\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his_hour=24\n",
    "pred_hour=6\n",
    "spot_id = 14208\n",
    "\n",
    "# 配置项\n",
    "exper_name = f\"mse_loss_his{his_hour}h-pred{pred_hour}h\"\n",
    "exper_dir = f\"exper/{exper_name}\"\n",
    "exper_data_dir = f\"exper_data/{exper_name}\"\n",
    "os.makedirs(exper_data_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = f\"{exper_data_dir}/raw/{spot_id}\"\n",
    "proc_dir = f\"{exper_data_dir}/proc/{spot_id}\"\n",
    "train_dir = f\"{exper_data_dir}/train/{spot_id}\"\n",
    "test_dir = f\"{exper_data_dir}/test/{spot_id}\"\n",
    "res_dir = f\"{exper_data_dir}/res/{spot_id}\"\n",
    "\n",
    "\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(proc_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)   \n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(res_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils_data import get_spot_config\n",
    "\n",
    "freq, his_len, pred_len = get_spot_config(spot_id, his_hour, pred_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据库连接成功！\n",
      "正在从表 'lingyin_passenger_flow' 中查询数据，时间范围: 2024-07-20 00:00:00 至 2025-07-20 23:59:59...\n",
      "输出表头: ['spot_id', 'kpi_time', 'kpi_value']\n",
      "CSV 文件表头已写入。\n",
      "成功！筛选后的数据已导出到文件 'exper_data/mse_loss_his24h-pred6h/raw/14208/14208_2024-07-20_2025-07-20.csv'，共 539145 条记录。\n",
      "数据库连接已关闭。\n"
     ]
    }
   ],
   "source": [
    "# 根据景点加载训练数据\n",
    "from src.utils.utils_eva_db import save_csv_from_db\n",
    "\n",
    "s_time = \"2024-07-20\"\n",
    "e_time = \"2025-07-20\"\n",
    "if e_time is None:\n",
    "    file_base_name = f\"{spot_id}_{s_time}\"\n",
    "    train_raw_data_file=f\"{raw_dir}/{file_base_name}.csv\"\n",
    "else:\n",
    "    file_base_name = f\"{spot_id}_{s_time}_{e_time}\"\n",
    "    train_raw_data_file=f\"{raw_dir}/{file_base_name}.csv\"\n",
    "save_csv_from_db(\n",
    "    spot_id=spot_id,\n",
    "    s_time=f\"{s_time} 00:00:00\",\n",
    "    e_time= f\"{e_time} 23:59:59\",\n",
    "    output_csv_file=train_raw_data_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_raw_data_file)\n",
    "df['kpi_time'] = pd.to_datetime(df['kpi_time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# 将kpi_time的秒清零\n",
    "df['kpi_time'] = df['kpi_time'].dt.floor('min')\n",
    "df = df.drop_duplicates(subset=['kpi_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spot_id</th>\n",
       "      <th>kpi_time</th>\n",
       "      <th>kpi_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14208</td>\n",
       "      <td>2024-07-20 00:00:00</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14208</td>\n",
       "      <td>2024-07-20 00:01:00</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14208</td>\n",
       "      <td>2024-07-20 00:02:00</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14208</td>\n",
       "      <td>2024-07-20 00:03:00</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14208</td>\n",
       "      <td>2024-07-20 00:04:00</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spot_id            kpi_time  kpi_value\n",
       "0    14208 2024-07-20 00:00:00        333\n",
       "1    14208 2024-07-20 00:01:00        333\n",
       "3    14208 2024-07-20 00:02:00        349\n",
       "4    14208 2024-07-20 00:03:00        349\n",
       "5    14208 2024-07-20 00:04:00        351"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "df = pd.read_csv(train_raw_data_file)\n",
    "# 按kpi_time列转换为datetime格式\n",
    "df['kpi_time'] = pd.to_datetime(df['kpi_time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "# 按kpi_time去重\n",
    "df = df.drop_duplicates(subset=['kpi_time'])\n",
    "# 按kpi_time排序\n",
    "df = df.sort_values(by='kpi_time')\n",
    "# 数据预处理，分景点\n",
    "if spot_id in [14210,14211,14212,14213]:\n",
    "    pass\n",
    "elif spot_id in [14207,14209]:\n",
    "    from src.utils.utils_data import fill_missing_value_singlespot_30s,preprocess_for_koopman_30s_moderate\n",
    "    df_proc = fill_missing_value_singlespot_30s(spot_id, df)\n",
    "    # 添加Koopman专用预处理\n",
    "    # df_proc = preprocess_for_koopman_30s_moderate(df_proc, spot_id)\n",
    "elif spot_id in [14208]:\n",
    "    df['kpi_time'] = df['kpi_time'].dt.floor('min')\n",
    "    df = df.drop_duplicates(subset=['kpi_time'])\n",
    "    from src.utils.utils_data import fill_missing_value_singlespot_day\n",
    "    df_proc = fill_missing_value_singlespot_day(df, freq=freq)  \n",
    "else:\n",
    "    from src.utils.utils_data import fill_missing_value_singlespot_day\n",
    "    df_proc = fill_missing_value_singlespot_day(df, freq=freq)\n",
    "\n",
    "df_proc.to_csv(f\"{proc_dir}/{file_base_name}_proc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting group annotation...\n",
      "Getting continuous groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building continuous groups: 100%|██████████| 444960/444960 [00:22<00:00, 19532.82row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 continuous groups\n",
      "Processing groups (his_len=1440, pred_len=360)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing continuous groups: 100%|██████████| 5/5 [05:00<00:00, 60.17s/group]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 287 holiday batches and 586 workday batches\n",
      "Final merging and deduplicating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging batches: 100%|██████████| 58/58 [00:03<00:00, 16.71it/s]\n",
      "Merging batches: 100%|██████████| 12/12 [00:00<00:00, 16.36it/s]\n",
      "Merging batches: 100%|██████████| 3/3 [00:00<00:00, 14.44it/s]\n",
      "Merging batches: 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n",
      "Merging batches: 100%|██████████| 118/118 [00:07<00:00, 16.41it/s]\n",
      "Merging batches: 100%|██████████| 24/24 [00:01<00:00, 16.47it/s]\n",
      "Merging batches: 100%|██████████| 5/5 [00:00<00:00, 14.45it/s]\n",
      "Merging batches: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final holiday data: 225634 rows\n",
      "Final workday data: 376920 rows\n",
      "Getting final continuous groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building continuous groups: 100%|██████████| 225634/225634 [00:12<00:00, 18317.68row/s]\n",
      "Building continuous groups: 100%|██████████| 376920/376920 [00:20<00:00, 18637.19row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final holiday groups: 45\n",
      "Final workday groups: 46\n",
      "Saving 45 groups to mode_0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving mode_0 files: 100%|██████████| 45/45 [00:00<00:00, 86.94file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode_0 data saved successfully!\n",
      "Saving 46 groups to mode_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving mode_1 files: 100%|██████████| 46/46 [00:00<00:00, 57.70file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode_1 data saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据处理为模型输入形式\n",
    "\n",
    "from src.pattern.pattern_train import get_group_annotation, save_mode_data\n",
    "df_proc = pd.read_csv(f\"{proc_dir}/{file_base_name}_proc.csv\")\n",
    "save_base_dir = train_dir\n",
    "os.makedirs(save_base_dir, exist_ok=True)\n",
    "groups_mode_0, groups_mode_1 = get_group_annotation(his_len=his_len,pred_len=pred_len, df=df_proc, time_interval=freq)\n",
    "save_mode_data(\n",
    "    groups_mode=groups_mode_0,\n",
    "    mode=0,\n",
    "    data_basepath=save_base_dir,\n",
    ")\n",
    "save_mode_data(\n",
    "    groups_mode=groups_mode_1,\n",
    "    mode=1,\n",
    "    data_basepath=save_base_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20000,\n",
       " 20001,\n",
       " 20002,\n",
       " 20003,\n",
       " 20004,\n",
       " 20005,\n",
       " 20006,\n",
       " 20007,\n",
       " 20008,\n",
       " 20009,\n",
       " 20010,\n",
       " 20011,\n",
       " 20012,\n",
       " 20013,\n",
       " 20014,\n",
       " 20015,\n",
       " 20016,\n",
       " 20017,\n",
       " 20018,\n",
       " 20019,\n",
       " 20020,\n",
       " 20021,\n",
       " 20022,\n",
       " 20023,\n",
       " 20024,\n",
       " 20025]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20000到20025的列表\n",
    "[20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
