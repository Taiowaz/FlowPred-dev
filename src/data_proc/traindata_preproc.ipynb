{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# 魔术指令，自动加载模块\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/home/beihang/xihu/HZTourism/FlowPred-dev\")\n",
    "import sys\n",
    "sys.path.append(\"/home/beihang/xihu/HZTourism/FlowPred-dev\")\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "his_hour=24\n",
    "pred_hour=6\n",
    "spot_id = 14207\n",
    "\n",
    "# 配置项\n",
    "exper_name = f\"mse_loss_his{his_hour}h-pred{pred_hour}h\"\n",
    "exper_dir = f\"exper/{exper_name}\"\n",
    "exper_data_dir = f\"exper_data/{exper_name}\"\n",
    "os.makedirs(exper_data_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = f\"{exper_data_dir}/raw/{spot_id}\"\n",
    "proc_dir = f\"{exper_data_dir}/proc/{spot_id}\"\n",
    "train_dir = f\"{exper_data_dir}/train/{spot_id}\"\n",
    "test_dir = f\"{exper_data_dir}/test/{spot_id}\"\n",
    "res_dir = f\"{exper_data_dir}/res/{spot_id}\"\n",
    "\n",
    "\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(proc_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)   \n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(res_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils_data import get_spot_config\n",
    "\n",
    "freq, his_len, pred_len = get_spot_config(spot_id, his_hour, pred_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据库连接成功！\n",
      "正在从表 'dahua_flow' 中查询数据，时间范围: 2024-07-20 00:00:00 至 2025-07-20 23:59:59...\n",
      "输出表头: ['spot_id', 'kpi_time', 'kpi_value']\n",
      "CSV 文件表头已写入。\n",
      "成功！筛选后的数据已导出到文件 'exper_data/mse_loss_his24h-pred6h/raw/14207/14207_2024-07-20_2025-07-20.csv'，共 1290115 条记录。\n",
      "数据库连接已关闭。\n"
     ]
    }
   ],
   "source": [
    "# 根据景点加载训练数据\n",
    "from src.utils.utils_eva_db import save_csv_from_db\n",
    "\n",
    "s_time = \"2024-07-20\"\n",
    "e_time = \"2025-07-20\"\n",
    "if e_time is None:\n",
    "    file_base_name = f\"{spot_id}_{s_time}\"\n",
    "    train_raw_data_file=f\"{raw_dir}/{file_base_name}.csv\"\n",
    "else:\n",
    "    file_base_name = f\"{spot_id}_{s_time}_{e_time}\"\n",
    "    train_raw_data_file=f\"{raw_dir}/{file_base_name}.csv\"\n",
    "save_csv_from_db(\n",
    "    spot_id=spot_id,\n",
    "    s_time=f\"{s_time} 00:00:00\",\n",
    "    e_time= f\"{e_time} 23:59:59\",\n",
    "    output_csv_file=train_raw_data_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "df = pd.read_csv(train_raw_data_file)\n",
    "# 按kpi_time列转换为datetime格式\n",
    "df['kpi_time'] = pd.to_datetime(df['kpi_time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "# 按kpi_time去重\n",
    "df = df.drop_duplicates(subset=['kpi_time'])\n",
    "# 按kpi_time排序\n",
    "df = df.sort_values(by='kpi_time')\n",
    "# 数据预处理，分景点\n",
    "if spot_id in [14210,14211,14212,14213]:\n",
    "    pass\n",
    "elif spot_id in [14207,14209]:\n",
    "    from src.utils.utils_data import fill_missing_value_singlespot_30s,preprocess_for_koopman_30s_moderate\n",
    "    df_proc = fill_missing_value_singlespot_30s(spot_id, df)\n",
    "    # 添加Koopman专用预处理\n",
    "    # df_proc = preprocess_for_koopman_30s_moderate(df_proc, spot_id)\n",
    "elif spot_id in [14208]:\n",
    "    pass\n",
    "else:\n",
    "    from src.utils.utils_data import fill_missing_value_singlespot_day\n",
    "    df_proc = fill_missing_value_singlespot_day(df, freq=freq)\n",
    "\n",
    "df_proc.to_csv(f\"{proc_dir}/{file_base_name}_proc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting group annotation...\n",
      "Getting continuous groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building continuous groups:   0%|          | 1/1032082 [00:00<172:09:17,  1.67row/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building continuous groups: 100%|██████████| 1032082/1032082 [00:49<00:00, 20791.19row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 481 continuous groups\n",
      "Processing groups (his_len=2880, pred_len=720)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing continuous groups: 100%|██████████| 481/481 [09:32<00:00,  1.19s/group] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 470 holiday batches and 937 workday batches\n",
      "Final merging and deduplicating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging batches: 100%|██████████| 94/94 [00:07<00:00, 12.97it/s]\n",
      "Merging batches: 100%|██████████| 19/19 [00:01<00:00, 12.81it/s]\n",
      "Merging batches: 100%|██████████| 4/4 [00:00<00:00, 11.52it/s]\n",
      "Merging batches: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n",
      "Merging batches: 100%|██████████| 188/188 [00:14<00:00, 13.02it/s]\n",
      "Merging batches: 100%|██████████| 38/38 [00:03<00:00, 12.61it/s]\n",
      "Merging batches: 100%|██████████| 8/8 [00:00<00:00, 11.56it/s]\n",
      "Merging batches: 100%|██████████| 2/2 [00:00<00:00,  8.62it/s]\n",
      "Merging batches: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final holiday data: 443020 rows\n",
      "Final workday data: 744790 rows\n",
      "Getting final continuous groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building continuous groups: 100%|██████████| 443020/443020 [00:23<00:00, 18598.03row/s]\n",
      "Building continuous groups: 100%|██████████| 744790/744790 [00:39<00:00, 19030.35row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final holiday groups: 57\n",
      "Final workday groups: 76\n",
      "Saving 57 groups to mode_0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving mode_0 files: 100%|██████████| 57/57 [00:00<00:00, 59.94file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode_0 data saved successfully!\n",
      "Saving 76 groups to mode_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving mode_1 files: 100%|██████████| 76/76 [00:01<00:00, 48.30file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode_1 data saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据处理为模型输入形式\n",
    "\n",
    "from src.pattern.pattern_train import get_group_annotation, save_mode_data\n",
    "df_proc = pd.read_csv(f\"{proc_dir}/{file_base_name}_proc.csv\")\n",
    "save_base_dir = train_dir\n",
    "os.makedirs(save_base_dir, exist_ok=True)\n",
    "groups_mode_0, groups_mode_1 = get_group_annotation(his_len=his_len,pred_len=pred_len, df=df_proc, time_interval=freq)\n",
    "save_mode_data(\n",
    "    groups_mode=groups_mode_0,\n",
    "    mode=0,\n",
    "    data_basepath=save_base_dir,\n",
    ")\n",
    "save_mode_data(\n",
    "    groups_mode=groups_mode_1,\n",
    "    mode=1,\n",
    "    data_basepath=save_base_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20000,\n",
       " 20001,\n",
       " 20002,\n",
       " 20003,\n",
       " 20004,\n",
       " 20005,\n",
       " 20006,\n",
       " 20007,\n",
       " 20008,\n",
       " 20009,\n",
       " 20010,\n",
       " 20011,\n",
       " 20012,\n",
       " 20013,\n",
       " 20014,\n",
       " 20015,\n",
       " 20016,\n",
       " 20017,\n",
       " 20018,\n",
       " 20019,\n",
       " 20020,\n",
       " 20021,\n",
       " 20022,\n",
       " 20023,\n",
       " 20024,\n",
       " 20025]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20000到20025的列表\n",
    "[20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
