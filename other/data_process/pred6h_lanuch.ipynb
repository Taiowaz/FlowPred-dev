{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 魔术指令，自动加载模块\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/home/beihang/xihu/HZTourism/FlowPred-dev\")\n",
    "from utils.utils_data import save_csv_from_db\n",
    "import pandas as pd\n",
    "from utils.utils_data import fill_missing_value_singlespot_day\n",
    "from pattern.pattern_train import get_group_annotation, save_mode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始数据获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_lst= [\"mobile_signaling_tourists_num\",\"dahua_flow\",\"lingyin_passenger_flow\"]\n",
    "\n",
    "for table_name in table_name_lst:\n",
    "    if table_name==\"mobile_signaling_tourists_num\":\n",
    "        time_col = \"kpi_time\"\n",
    "    elif table_name==\"dahua_flow\":\n",
    "        time_col = \"date_time\"\n",
    "    save_csv_from_db(\n",
    "        table_name=table_name,\n",
    "        time_col=time_col,\n",
    "        s_time=\"2024-07-19 00:00:00\",\n",
    "        e_time=\"2025-07-20 00:00:00\",\n",
    "        output_csv_file=f\"data/pred6h_launch/raw/{table_name}_240719_250720.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 信令客流处理\n",
    "df_mobile = pd.read_csv(\"data/pred6h_launch/raw/mobile_signaling_tourists_num_240719_250720.csv\")\n",
    "df_mobile[\"kpi_time\"] = pd.to_datetime(df_mobile[\"kpi_time\"])\n",
    "df_mobile.drop_duplicates(subset=[\"spot_id\",\"kpi_time\"], inplace=True)\n",
    "spot_id_lst = [14100,14102,14103,14105,14107,14108,14114,14115,14116,14120,14124,14125,14126,14127,14129,14137,14141,14144,14145,14205]\n",
    "\n",
    "for spot_id in spot_id_lst:\n",
    "    df_mobile_spot = df_mobile[df_mobile[\"spot_id\"]==spot_id]\n",
    "    df_mobile_spot = df_mobile_spot[[\"spot_id\",\"kpi_time\",\"kpi_value\"]]\n",
    "    df_mobile_spot = fill_missing_value_singlespot_day(df_mobile_spot)\n",
    "    df_mobile_spot.to_csv(f\"data/pred6h_launch/raw/mobile_spot/{spot_id}_240719_250720.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充函数\n",
    "def fill_missing_value_singlespot_30s(spot_id,df):\n",
    "    df['minute_str'] = df[\"kpi_time\"].dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df_groups = list(df.groupby('minute_str'))\n",
    "    kpi_time_lst = []\n",
    "    kpi_value_lst = []\n",
    "    for minute_str, group in df_groups:\n",
    "        if len(group) == 1:\n",
    "            # 依据minute_str生成两个时间戳，其他都一样，秒数一个是00，一个是30\n",
    "            kpi_time_lst.append(group['kpi_time'].iloc[0].replace(second=0, microsecond=0))\n",
    "            kpi_time_lst.append(group['kpi_time'].iloc[0].replace(second=30, microsecond=0))\n",
    "            kpi_value_lst.append(group['kpi_value'].iloc[0])\n",
    "            kpi_value_lst.append(group['kpi_value'].iloc[0])\n",
    "        elif len(group) == 2:\n",
    "            kpi_time_lst.append(group['kpi_time'].iloc[0].replace(second=0, microsecond=0))\n",
    "            kpi_time_lst.append(group['kpi_time'].iloc[0].replace(second=30, microsecond=0))\n",
    "            kpi_value_lst.append(group['kpi_value'].iloc[0])\n",
    "            kpi_value_lst.append(group['kpi_value'].iloc[1])\n",
    "        elif len(group) > 2:\n",
    "            df_s0 = group[group['kpi_time'].dt.second == 0]\n",
    "            df_s30 = group[group['kpi_time'].dt.second == 30]\n",
    "            # 初始化为 \n",
    "            kpi_value_0 = None\n",
    "            kpi_value_30 = None\n",
    "            if len(df_s0) > 0:\n",
    "                kpi_value_0 = df_s0['kpi_value'].iloc[0]\n",
    "            if len(df_s30) > 0:\n",
    "                kpi_value_30 = df_s30['kpi_value'].iloc[0]\n",
    "            if kpi_value_0 is None:\n",
    "                kpi_value_0 = group['kpi_value'].iloc[0]\n",
    "            if kpi_value_30 is None:\n",
    "                kpi_value_30 = group['kpi_value'].iloc[-1]\n",
    "            kpi_time_lst.append(group['kpi_time'].iloc[0].replace(second=0, microsecond=0))\n",
    "            kpi_time_lst.append(group['kpi_time'].iloc[0].replace(second=30, microsecond=0))\n",
    "            kpi_value_lst.append(kpi_value_0)\n",
    "            kpi_value_lst.append(kpi_value_30)\n",
    "    df_res = pd.DataFrame({\n",
    "        'spot_id': [spot_id] * len(kpi_time_lst),\n",
    "        'kpi_time': kpi_time_lst,\n",
    "        'kpi_value': kpi_value_lst\n",
    "    })\n",
    "    df_res['kpi_time'] = pd.to_datetime(df_res['kpi_time'])\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 断桥数据初始处理\n",
    "df_dh = pd.read_csv(\"data/pred6h_launch/raw/dahua_flow_240719_250720.csv\")\n",
    "df_dh = df_dh[[\"date_time\",\"num\"]]\n",
    "df_dh.rename(columns={\"date_time\": \"kpi_time\", \"num\": \"kpi_value\"}, inplace=True)\n",
    "df_dh[\"spot_id\"] = 14207\n",
    "df_dh[\"kpi_time\"] = pd.to_datetime(df_dh[\"kpi_time\"])\n",
    "df_dh.drop_duplicates(subset=[\"kpi_time\"], inplace=True)\n",
    "df_dh['minute_str'] = df_dh[\"kpi_time\"].dt.strftime('%Y-%m-%d %H:%M')\n",
    "df_dh = df_dh.sort_values(['minute_str', 'kpi_value']).drop_duplicates(['minute_str', 'kpi_value'])\n",
    "df_dh_filled = fill_missing_value_singlespot_30s(14207, df_dh)\n",
    "df_dh_filled.to_csv(\"data/pred6h_launch/raw/proc/14207_240719_250720.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 灵隐寺数据初始处理\n",
    "df_ly = pd.read_csv(\"data/pred6h_launch/raw/lingyin_passenger_flow_240719_250720.csv\")\n",
    "df_ly = df_ly[[\"date_time\",\"real_time_num\"]]\n",
    "df_ly.rename(columns={\"date_time\": \"kpi_time\", \"real_time_num\": \"kpi_value\"}, inplace=True)\n",
    "df_ly[\"spot_id\"] = 14208\n",
    "df_ly[\"kpi_time\"] = pd.to_datetime(df_ly[\"kpi_time\"])\n",
    "df_ly.drop_duplicates(subset=[\"kpi_time\"], inplace=True)\n",
    "df_ly['minute_str'] = df_ly[\"kpi_time\"].dt.strftime('%Y-%m-%d %H:%M')\n",
    "df_ly = df_ly.sort_values(['minute_str', 'kpi_value']).drop_duplicates(['minute_str', 'kpi_value'])\n",
    "df_ly_filled = fill_missing_value_singlespot_30s(14208, df_ly)\n",
    "df_ly_filled.to_csv(\"data/pred6h_launch/raw/proc/14208_240719_250720.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按分钟分组，之后统计每分钟的数据个数，生成一个字典保存在temp文件夹下\n",
    "def group_by_minute_str(df):\n",
    "    time_col = \"kpi_time\"\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    # 以“年月日时分”字符串为分组依据\n",
    "    df['minute_str'] = df[time_col].dt.strftime('%Y-%m-%d %H:%M')\n",
    "    count_per_minute = df.groupby('minute_str').size()\n",
    "    # 保存在temp文件夹下\n",
    "    count_per_minute.to_csv(\"temp/count_per_minute.csv\", header=True)\n",
    "    count_distribution = count_per_minute.value_counts().sort_index().to_dict()\n",
    "    return count_per_minute, count_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型输入处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_dir = \"data/pred6h_lanuch/raw/proc\"\n",
    "# df_files = sorted(os.listdir(df_dir))\n",
    "df_files = [\"14207_240719_250720.csv\", \"14208_240719_250720.csv\"]\n",
    "\n",
    "for df_file in df_files:\n",
    "    spot_id = int(df_file.split('_')[0]) \n",
    "    his_len = 288\n",
    "    pred_len = 72\n",
    "    time_interval=\"5min\"\n",
    "    if spot_id in [14207, 14208]:\n",
    "        his_len = 2880\n",
    "        pred_len = 720\n",
    "        time_interval=\"30s\"\n",
    "    data_save_basedir = f\"data/pred6h_lanuch/input/{str(spot_id)}\"\n",
    "    df = pd.read_csv(os.path.join(df_dir, df_file))\n",
    "    groups_mode_0, groups_mode_1 = get_group_annotation(his_len=his_len,pred_len=pred_len, df=df, time_interval=time_interval)\n",
    "    save_mode_data(\n",
    "        groups_mode=groups_mode_0,\n",
    "        mode=0,\n",
    "        his_len=his_len,\n",
    "        pred_len=pred_len,\n",
    "        data_basepath=data_save_basedir,\n",
    "    )\n",
    "    save_mode_data(\n",
    "        groups_mode=groups_mode_1,\n",
    "        mode=1,\n",
    "        his_len=his_len,\n",
    "        pred_len=pred_len,\n",
    "        data_basepath=data_save_basedir,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
